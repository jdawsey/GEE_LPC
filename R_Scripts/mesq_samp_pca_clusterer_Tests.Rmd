---
title: "mesq_samp_naip"
author: "Justin Dawsey"
date: "2023-11-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
###ee_install()

library(rgee)
library(rgeeExtra)
library(reticulate)

# Initialize Earth Engine and GD
#ee_Initialize()
ee_Initialize(drive=TRUE)

```

```{r}
library(sf)
library(sp)
library(geojsonsf)
library(rlist)
library(dplyr)
# read in each geojson


files_list <- list.files(path="C:/Users/Justin/Desktop/mesquite/mesq_jsons")

# creating a path for each item in a file
file_address_func <- function(file_given) {
  addresses <- list()
  
  for (file in file_given) {
    address <- "C:/Users/Justin/Desktop/mesquite/mesq_jsons/"
    full_address <- paste0(address, file)
    length_list <- length(addresses)
    addresses <- append(addresses, full_address, after = length_list)
  }
  return(addresses)
}

listy <- file_address_func(files_list)
#listy
```


```{r}
# generating names for each object to assign an sf value
object_names <- function(given_list) {
  ob_list <- list()
  
  for (listicle in given_list) {
    temp_string <- "buff"
    ob_name <- paste0(listicle, temp_string)
    length_list <- length(ob_list)
    ob_list <- append(ob_list, ob_name, after = length_list)
    
  }
  return(ob_list)
}

ob_names <- object_names(files_list)
#ob_names
```



```{r}
# creating a function to save all the geojsons to sf objects
rng <- 1:length(listy)
geo_json_func <- function(given_list, given_range) {
  json_sf_list <- list()
  
  for (item in given_range) {
    full_address <- given_list[[item]]
    len_list <- length(json_sf_list)
    json_sf_list <- append(json_sf_list, geojson_sf(full_address), after = length(json_sf_list))
  }
  return(json_sf_list)
}  
json_list <- geo_json_func(listy, rng)
json_list[[1]]


# adding sf objects as ee assets
ee_list_func <- function(given_json_list, given_range) {
  eeItemList <- list()
  
  for (item in given_range) {
    eeItemList <- append(eeItemList,
                         sf_as_ee(
                           x = json_list[[item]],
                           assetID = ob_names[[item]]
                         ),
                         after = length(eeItemList)
                         )
    
  }
  return(eeItemList)
}

eeItemList <- ee_list_func(json_list, rng)
#eeItemList

```
# ###############################################
# ###############################################
# ###############################################
# ###############################################
# ###############################################

Loading the NAIP Imagery

Years - NM - 2009 (1m RGB), 2011 (1m), 2014 (1m), 2016 (1m), 2018, 2020, 2022
      - TX - 2004 (1m NRG), 2008 (1m NRG), 2010 (1m) , 2012 (1m), 2014 (1m), 2016 (1m), 2018, 2020, 2022

```{r}
### change the feature collection to your boundary asset location
shp <- eeItemList[[10]] # change per buffer ###########havent run 11 yet

### choose your imagery and the dates for it
year <- ee$ImageCollection('USDA/NAIP/DOQQ') %>%
      ee$ImageCollection$filterDate(ee$Date('2016-01-01'), 
                                    ee$Date('2016-12-31')) %>%
      ee$ImageCollection$filterBounds(shp)

# mosaic the collection into a single image
year <- year$mosaic()

### clipping to shape
clip <- year$clip(shp)
###

### visibility parameters for the color image
visParam <- list(bands <- c('R', 'G', 'B'),
  gamma = 1
  )

# centering and adding to map to check that it's visualizing properly
#Map$centerObject(shp)
#Map$addLayer(clip, visParams= visParam)
```

Normalization

```{r}

# Get mean and SD in every band by combining reducers.
stats <- clip$reduceRegion(
  reducer = ee$Reducer$mean()$combine(
    reducer2 = ee$Reducer$stdDev(),
    sharedInputs = TRUE
  ),
  geometry = shp,
  scale = 1, # change depending on year
  bestEffort = TRUE # Use maxPixels if you care about scale.
)

#print(stats$getInfo())

# Extract means and SDs to images.
meansImage <- stats$toImage()$select('.*_mean')
sdsImage <- stats$toImage()$select('.*_stdDev')
coeffVarImage <- meansImage$divide(sdsImage)

normalized = clip$subtract(coeffVarImage)


######## Visualize if wanted ################
#visParamNorm <- list(bands <- c('R', 'G', 'B'),
#  min= 0,
#  max= 255
#  )

# Map$addLayer(normalized, visParams = visParamNorm)
###
```

Adding indices for additional possible accuracy

https://isprs-archives.copernicus.org/articles/XLII-3/1215/2018/isprs-archives-XLII-3-1215-2018.pdf
EVALUATION OF RGB-BASED VEGETATION INDICES FROM UAV IMAGERY TO
ESTIMATE FORAGE YIELD IN GRASSLAND

VARI (Visible Atmospherically Resistant Index):
Gitelson AA, Vina A, Arkebauer TJ, Rundquist DC, Keydan G, Leavitt B. Remote estimation of leaf area index and green leaf biomass in maize canopies. Geophys Res Lett. 2003;30(30):335â€“43. https://doi.org/10.1029/2002gl016450.

# Commenting out since have NIR band

```{r}
#Doesn't work for NM 2009 -- see "check bands"
#band_names <- normalized$bandNames()
#print(band_names$get(4)$getInfo())

rgb_vari <- function(image_given) {
  image_bands <- image_given$select(c("R", "G", "B"))
  image <- image_bands
  
  vari <- image$expression(
    expression = '(G - R) / (G + R - B)',
    opt_map =  list(
        'G' = image$select('G'),
        'R' = image$select('R'),
        'B' = image$select('B')
        )
    )$rename('vari')
  return(vari)
}

#vari <- rgb_vari(normalized)

# adding vari as a band to use
#normalized <- normalized$addBands(vari)

# checking was added
#band_names <- normalized$bandNames()
#print(band_names$get(4)$getInfo()) 
```

SAVI (Soil Adjusted Vegetation Index):
SAVI = (1 + L) * (Bnir - Bred) / (Bnir + Bred + L)
--- L = correction factor (0 = high veg, 1 = very little veg)

```{r}
rgb_savi <- function(image_given) {
  image_bands <- image_given$select(c("R", "N"))
  image <- image_bands
  
  savi <- image$expression(
    expression = '(1 + 0.6) * (N - R) / (N + R + 0.6)',
    opt_map =  list(
        'R' = image$select('R'),
        'N' = image$select('N')
        )
    )$rename('savi')
  return(savi)
}

savi <- rgb_savi(normalized)
#Map$addLayer(savi)

# adding savi as a band to use
normalized <- normalized$addBands(savi)

# checking was added
band_names <- normalized$bandNames()
#print(band_names$get(4)$getInfo()) 
```

ENDVI (Enhanced Normalized Difference Vegetation Index):
ENDVI = ((NIR + Green) - (2 * Blue)) / ((NIR + Green) + (2 * Blue))

```{r}
rgb_endvi <- function(image_given) {
  image_bands <- image_given$select(c("N", "G", "B"))
  image <- image_bands
  
  endvi <- image$expression(
    expression = '((N + G) - (2 * B)) / ((N + G) + (2 + B))',
    opt_map =  list(
        'N' = image$select('N'),  
        'G' = image$select('G'),
        'B' = image$select('B')
        )
    )$rename('endvi')
  return(endvi)
}

endvi <- rgb_endvi(normalized)

# adding endvi as a band to use
normalized <- normalized$addBands(endvi)

# checking was added
band_names <- normalized$bandNames()
#print(band_names$get(5)$getInfo()) 
```

Running a correlation test on the bands and indices. May be worth running a PCA analysis.

```{r}
########################
########################
########################
ee_print(normalized)


pca_test <- function(image_given) {
  image = image_given$unmask()
  scale = 1
  region = shp
  bandNames = c('R', 'G', 'B', 'N', 'savi', 'endvi')
  #Mean center the data to enable a faster covariance reducer
  #and an SD stretch of the principal components.
  meanDict = image$reduceRegion(list(
    reducer = ee$Reducer$mean(),
    #geometry = region,
    scale = scale,
    maxPixels = 200000000,
    tileScale = 8
  ))
  
  meanssss = ee$Image$constant(meanDict$values(c('R', 'G', 'B', 'N', 'savi', 'endvi')))
  centered = image$subtract(meanssss)
  
  #This helper function returns a list of new band names.
  #adds the prefix 'pc' to the numbers being created by the pca.
  getNewBandNames <- function(prefix) {
    sequ <- ee$List$sequence(1, bandNames$length())
    sequ_map <- sequ$map(ee_utils_pyfunc(function(b) {
      return(ee$String(prefix)$cat(ee$Number(b)$int()))
    }))
    return(ee_print(sequ_map))
  }
  
  #This function accepts mean centered imagery, a scale and
  #a region in which to perform the analysis.  It returns the
  #Principal Components (PC) in the region as a new image.
  getPrincipalComponents = function(centered, scale) {
    #Collapse the bands of the image into a 1D array per pixel.
    arrays <- centered$toArray()
    
    #Compute the covariance of the bands within the region.
    covar = arrays$reduceRegion(
      reducer = ee$Reducer$centeredCovariance(),
    )
    
    #Get the 'array' covariance result and cast to an array.
    #This represents the band-to-band covariance within the region.
    covarArray <- ee$Array(covar$get('array'))
    
    #Perform an eigen analysis and slice apart the values and vectors.
    eigens <- covarArray$eigen()
    
    #This is a P-length vector of Eigenvalues.
    eigenValues <- eigens$slice(1, 0, 1)
    
    #Compute Percentage Variance of each component
    #This will allow us to decide how many components capture
    #most of the variance in the input
    eigenValuesList <- eigenValues$toList()$flatten()
    total <- eigenValuesList$reduce(ee$Reducer$sum())
    
    
    percentageVariance <- eigenValuesList$map(ee_utils_pyfunc(function(item) {
      component <- eigenValuesList$indexOf(item)$add(1)$format('%02d')
      variance <- ee$Number(item)$divide(total)$multiply(100)$format('%.2f')
      
      return(ee$List(component, variance))
    }))
    
    #Create a dictionary that will be used to set properties on final image
    varianceDict <- ee$Dictionary(percentageVariance$flatten())
    #This is a PxP matrix with eigenvectors in rows.
    eigenVectors <- eigens$slice(1, 1)
    #Convert the array image to 2D arrays for matrix computations.
    arrayImage <- arrays$toArray(1)
    
    #Left multiply the image array by the matrix of eigenvectors.
    principalComponents <- ee$Image(eigenVectors)$matrixMultiply(arrayImage)
    
    #Turn the square roots of the Eigenvalues into a P-band image.
    #Call abs() to turn negative eigenvalues to positive before
    #taking the square root
    sdImage <- ee$Image(eigenValues$abs()$sqrt())$arrayProject((0))$arrayFlatten((getNewBandNames('sd')))
    
    #Turn the PCs into a P-band image, normalized by SD.
    #Throw out an an unneeded dimension, [[]] -> [].
    #Make the one band array image a multi-band image, [] -> image.
    #Normalize the PCs by their SDs.
    return(principalComponents$arrayProject((0))$arrayFlatten((getNewBandNames('pc')))$divide(sdImage)$set(varianceDict))
  }
  pcImage <- getPrincipalComponents(centered, scale)
  return(pcImage$mask(maskedImage$mask()))
}

bands_huh <- pca_test(normalized)
bands_huh

```

```{r}
image = normalized$unmask()
scale = 1
region = shp
bandNames = c('R', 'G', 'B', 'N', 'savi', 'endvi')
#Mean center the data to enable a faster covariance reducer
#and an SD stretch of the principal components.
meanDict = image$reduceRegion(
  reducer = ee$Reducer$mean(),
  #geometry = region,
  scale = scale,
  maxPixels = 200000000,
  tileScale = 8
)
  
meanssss = ee$Image$constant(meanDict$values(c('R', 'G', 'B', 'N', 'savi', 'endvi')))
centered = image$subtract(meanssss)
  
#This helper function returns a list of new band names.
#adds the prefix 'pc' to the numbers being created by the pca.
getNewBandNames <- function(prefix) {
  sequ <- ee$List$sequence(1, bandNames$length())
  sequ_map <- sequ$map(ee_utils_pyfunc(function(b) {
    return(ee$String(prefix)$cat(ee$Number(b)$int()))
  }))
  return(ee_print(sequ_map))
}


  
#This function accepts mean centered imagery, a scale and
#a region in which to perform the analysis.  It returns the
#Principal Components (PC) in the region as a new image.

#Collapse the bands of the image into a 1D array per pixel.
arrays <- centered$toArray()
    
#Compute the covariance of the bands within the region.
covar = arrays$reduceRegion(
  reducer = ee$Reducer$centeredCovariance(),
)
    
#Get the 'array' covariance result and cast to an array.
#This represents the band-to-band covariance within the region.
covarArray <- ee$Array(covar$get('array'))
    
#Perform an eigen analysis and slice apart the values and vectors.
eigens <- covarArray$eigen()
    
#This is a P-length vector of Eigenvalues.
eigenValues <- eigens$slice(1, 0, 1)
    
#Compute Percentage Variance of each component
#This will allow us to decide how many components capture
#most of the variance in the input
eigenValuesList <- eigenValues$toList()$flatten()
total <- eigenValuesList$reduce(ee$Reducer$sum())

    
######
###### Problem area
percentageVariance <- eigenValuesList$iterate(
    function(eigenValuesList) {
    item <- ee$Number(item)
    component <- eigenValuesList$indexOf(item)$add(1)$format('%02d')
    variance <- item$divide(total)$multiply(100)$format('%.2f')
      
    return(ee$List(component, variance))
  })
######
######

#Create a dictionary that will be used to set properties on final image
varianceDict <- ee$Dictionary(percentageVariance$flatten())
#This is a PxP matrix with eigenvectors in rows.
eigenVectors <- eigens$slice(1, 1)
#Convert the array image to 2D arrays for matrix computations.
arrayImage <- arrays$toArray(1)
    
#Left multiply the image array by the matrix of eigenvectors.
principalComponents <- ee$Image(eigenVectors)$matrixMultiply(arrayImage)
    
#Turn the square roots of the Eigenvalues into a P-band image.
#Call abs() to turn negative eigenvalues to positive before
#taking the square root
sdImage <- ee$Image(eigenValues$abs()$sqrt())$arrayProject((0))$arrayFlatten((getNewBandNames('sd')))
    
#Turn the PCs into a P-band image, normalized by SD.
#Throw out an an unneeded dimension, [[]] -> [].
#Make the one band array image a multi-band image, [] -> image.
#Normalize the PCs by their SDs.
pcImage <- principalComponents$arrayProject((0))$arrayFlatten((getNewBandNames('pc')))$divide(sdImage)$set(varianceDict)


bands_hub <- pcImage$mask(maskedImage$mask())
  

```
Re-normalizing

```{r}
# Get mean and SD in every band by combining reducers.
stats <- normalized$reduceRegion(
  reducer = ee$Reducer$mean()$combine(
    reducer2 = ee$Reducer$stdDev(),
    sharedInputs = TRUE
  ),
  geometry = shp,
  scale = 1, # change depending on year
  bestEffort = TRUE # Use maxPixels if you care about scale.
)

#print(stats$getInfo())

# Extract means and SDs to images.
#meansImage <- stats$toImage()$select('.*_mean')
#sdsImage <- stats$toImage()$select('.*_stdDev')
#coeffVarImage <- meansImage$divide(sdsImage)

#normalized = normalized$subtract(coeffVarImage)
```

Sharpening and smoothing

```{r}
### DoG sharpening
fat <- ee$Kernel$gaussian(
  radius = 3,
  sigma = 3,
  magnitude = -1.0,
  units = 'pixels'
)

skinny <- ee$Kernel$gaussian(
  radius = 3,
  sigma = 0.5,
  units = 'pixels'
)

dog <- fat$add(skinny)
sharpened <- normalized$add(normalized$convolve(dog))
###

### gaussian smoothing
gaussianKernel <- ee$Kernel$gaussian(
  radius = 3,
  units = 'pixels'
)

gauss <- sharpened$convolve(gaussianKernel)
###

visParamGauss <- list(bands <- c('R', 'G', 'B'),
  min= 0,
  max= 255
  )

### checking that image was smoothed
#Map$addLayer(gauss, visParams = visParamGauss)
```

Training the classification algorithm and showing the display parameters

```{r}

### creating training samples for the unsupervised classification
training <- gauss$sample(
  region = shp,
  scale = 1, #change depending on year
  numPixels = 10000
)
###

### the actual classification function
clusterer <- ee$Clusterer$wekaKMeans(10) %>% # change clusters depending on imagery
  ee$Clusterer$train(training)

result <- gauss$cluster(clusterer)
###

### creating a landcover palette to view the result
landcoverPalette <- c(
  '#00a944', 
  '#d6d6d6', 
  '#c65b8d', 
  '#000000', 
  '#ff3434', 
  '#5bb48f', 
  '#47f37c', 
  '#c04848', 
  '#ded132', 
  '#5bb48f', 
  '#0f0077', 
  '#778bff',
  '#8f8f8f',
  '#000000',
  '#d9a300'
  
)

visPalette <- list(
  min= 0,
  max= 9, # change depending on number of clusters
  palette = landcoverPalette
  )
###

### adding maps to compare
classified <- Map$addLayer(result, visParams = visPalette)
imageryMap <- Map$addLayer(clip, visParams= visParam)
###
```

Displaying the maps

```{r}
### showing the maps
classified | imageryMap
```

It may be worth just going ahead and making some training samples for each of the cover types I want to train for. That way a confusion matrix can automatically be generated.


```{r}
drive_image <- ee_image_to_drive(
  image = gauss,
  description = "export",
  folder = "!imagery",
  region = shp,
  scale = 1,
  max = 130000000
)

drive_image$start()
```


Downloading the classified map

```{r}
#shp_geo <- shp$geometry()

drive_image <- ee_image_to_drive(
  image = result,
  description = "export",
  folder = "!imagery",
  region = shp,
  scale = 1,
  max = 130000000
)

drive_image$start()
```
